{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col, desc, lit, when, count,concat, sum, asc, regexp_replace, explode, split, row_number\n",
    "\n",
    "# Spark Inicialization\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".master(\"local[*]\") \\\n",
    ".config('spark.jars', '/home/semantix/alertas/volume/postgresql-42.2.9.jre6.jar')\\\n",
    ".appName(\"teste\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "NOT_IMPL_MSG = \"This method need to be implemented, please check the documentation\"\n",
    "\n",
    "class DAO:\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def select(self, db_table):\n",
    "        raise NotImplementedError(NOT_IMPL_MSG)\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def insert(db_table, dataframe):\n",
    "        raise NotImplementedError(NOT_IMPL_MSG)\n",
    "        \n",
    "    @staticmethod\n",
    "    def from_csv(path, header=True):\n",
    "        return spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", header).load(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def instanceOf(subclass):\n",
    "        try:\n",
    "            return next(filter(lambda x: x.__name__ == subclass, DAO.__subclasses__()))\n",
    "        except StopIteration as error:\n",
    "            raise Exception(f'Please verify if this feature was implemented: {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "class PostgresDAO(DAO):\n",
    "    def __init__(self):\n",
    "        \"\"\"API to access Postgres RDBMS - DAO Implementation\"\"\"\n",
    "        \n",
    "        self.host = socket.gethostbyname(socket.gethostname())\n",
    "        self.database = \"postgres\"\n",
    "        self.user = \"postgres\"\n",
    "        self.password = \"postgres\"\n",
    "        self.port = 5432\n",
    "        \n",
    "    def select(self, db_table):\n",
    "        \"\"\"Query engine to this DAO implementation\"\"\"\n",
    "        try:\n",
    "            return spark.read\\\n",
    "            .format('jdbc')\\\n",
    "            .option('url', f'jdbc:postgresql://{self.host}:{self.port}/{self.database}')\\\n",
    "            .option('dbtable', f'{db_table}')\\\n",
    "            .option('user', self.user)\\\n",
    "            .option('password', self.password)\\\n",
    "            .option('driver', 'org.postgresql.Driver')\\\n",
    "            .load()     \n",
    "        except Exception as error: \n",
    "            logger.error(error)\n",
    "            \n",
    "    def insert(self, db_table, dataframe):\n",
    "        \"\"\"Data ingestion engine to this DAO implementation\"\"\"\n",
    "        try:\n",
    "            dataframe.write\\\n",
    "            .format('jdbc')\\\n",
    "            .option('url', f'jdbc:postgresql://{self.host}:{self.port}/{self.database}')\\\n",
    "            .option('dbtable', f'{db_table}')\\\n",
    "            .option('user', self.user)\\\n",
    "            .option('password', self.password)\\\n",
    "            .option('driver', 'org.postgresql.Driver')\\\n",
    "            .mode('append')\\\n",
    "            .save()\n",
    "        except Exception as error: \n",
    "            logger.error(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "NOT_IMPL_MSG = 'This method need to be implemented, please check the documentation'\n",
    "\n",
    "class Ingestor:\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def start(self):\n",
    "        raise NotImplementedError(NOT_IMPL_MSG)\n",
    "    \n",
    "    def load(self, metadata, data):\n",
    "        try:\n",
    "            sink = metadata.get('sink')\n",
    "            db_table = f'{metadata.get(\"database\")}.{metadata.get(\"table\")}'\n",
    "            access = DAO.instanceOf(sink)\n",
    "            access().insert(db_table=db_table, dataframe=data)\n",
    "        except Exception as error: \n",
    "            logger.error(error)\n",
    "            \n",
    "    @staticmethod\n",
    "    def apply(f, metadata):\n",
    "        return f(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, when, lit\n",
    "\n",
    "\n",
    "class Dimension(Ingestor):\n",
    "    def __init__(self, source, model, field, database='default', table='table', sink='HiveDAO', embbebedList=False):\n",
    "        self.metadata = dict({\n",
    "            'source': source,\n",
    "            'database': database,\n",
    "            'table': table,\n",
    "            'sink': sink,\n",
    "            'model': model,\n",
    "            'embbebedList': embbebedList,\n",
    "            'description_field': field})\n",
    "\n",
    "    def save(self):\n",
    "        data = Ingestor.apply(Dimension.rule, self.metadata)\n",
    "        self.load(self.metadata, data)\n",
    "\n",
    "    def get(self):\n",
    "        return Ingestor.apply(Dimension.rule, self.metadata)\n",
    "\n",
    "    @staticmethod\n",
    "    def checkConstraint(field, dataframe):\n",
    "        \"\"\" Responsible for verifying the field's integrity given dataframe\"\"\"\n",
    "        return dataframe.\\\n",
    "            withColumn(field, when(col(field).isNull(), lit('Not Specified'))\\\n",
    "            .otherwise(col(field)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_index(field, source):\n",
    "        return map(lambda x, y: (x+1, y.asDict()[field]), range(0, len(source)), source)\n",
    "\n",
    "    @staticmethod\n",
    "    def rule(metadata):\n",
    "        \"\"\" Responsible to implement business logic to dimensional tables \"\"\"\n",
    "        model = metadata.get('model')\n",
    "        field = metadata.get('description_field')\n",
    "\n",
    "        if metadata.get('embbebedList') == False:\n",
    "            data = Dimension.checkConstraint(field=field, dataframe=metadata.get('source'))\\\n",
    "                .select(field)\\\n",
    "                .distinct()\\\n",
    "                .collect()\n",
    "        else:\n",
    "            data = Dimension.checkConstraint(field=field, dataframe=metadata.get('source'))\\\n",
    "                .select(field)\\\n",
    "                .withColumn(field, split(col(field), ';'))\\\n",
    "                .withColumn(field, explode(col(field)))\\\n",
    "                .distinct()\\\n",
    "                .collect()\n",
    "\n",
    "        return spark.createDataFrame(Dimension.create_index(field, data), model().schema)\n",
    "\n",
    "    @staticmethod\n",
    "    def build(datasource, metadata):\n",
    "        return Dimension(source=datasource,\n",
    "            model=metadata.get('model'),\n",
    "            field=metadata.get('field'),\n",
    "            database=metadata.get('database'),\n",
    "            table=metadata.get('table'),\n",
    "            embbebedList=metadata.get('embbebedList'),\n",
    "            sink=metadata.get('sink'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StringType, IntegerType, StringType, StructType, DoubleType\n",
    "\n",
    "class CommunicationToolModel:\n",
    "    @property\n",
    "    def schema(self):    \n",
    "        return StructType([\n",
    "            StructField('ferramenta_comunic_id', IntegerType(), False),\n",
    "            StructField('nome', StringType(), False)])\n",
    "\n",
    "class SOModel(StructType):\n",
    "    @property\n",
    "    def schema(self):\n",
    "        return StructType([\n",
    "                    StructField('sistema_operacional_id', IntegerType(), False),\n",
    "                    StructField('nome', StringType(), False)])\n",
    "    \n",
    "class LanguageWorkedWithModel:\n",
    "    @property\n",
    "    def schema(self):    \n",
    "        return StructType([\n",
    "            StructField('linguagem_programacao_id', IntegerType(), False),\n",
    "            StructField('nome', StringType(), False)])\n",
    "    \n",
    "class CompanyModel:\n",
    "    @property\n",
    "    def schema(self):    \n",
    "        return StructType([\n",
    "            StructField('empresa_id', IntegerType(), False),\n",
    "            StructField('tamanho', StringType(), False)])\n",
    "\n",
    "class CountryModel:\n",
    "    @property\n",
    "    def schema(self):    \n",
    "        return StructType([\n",
    "            StructField('pais_id', IntegerType(), False),\n",
    "            StructField('nome', StringType(), False)])\n",
    "\n",
    "class RespLanguageModel:\n",
    "    @property\n",
    "    def schema(self):    \n",
    "        return StructType([\n",
    "            StructField('resp_usa_linguagem_id', IntegerType(), False),\n",
    "            StructField('respondente_id', IntegerType(), False),\n",
    "            StructField('linguagem_programacao_id', IntegerType(), False),\n",
    "            StructField('momento', IntegerType(), False)])\n",
    "    \n",
    "class RespToolsModel:\n",
    "    @property\n",
    "    def schema(self):    \n",
    "        return StructType([\n",
    "            StructField('resp_usa_ferramenta_id', IntegerType(), False),\n",
    "            StructField('respondente_id', IntegerType(), False),\n",
    "            StructField('ferramenta_comunic_id', IntegerType(), False)])\n",
    "    \n",
    "class RespondentModel:\n",
    "    @property\n",
    "    def schema(self):    \n",
    "        return StructType([\n",
    "            StructField(\"respondente_id\", IntegerType(), False),\n",
    "            StructField(\"nome\", StringType(), False),\n",
    "            StructField(\"contrib_open_source\", IntegerType(), False),\n",
    "            StructField(\"programa_hobby\", IntegerType(), False),\n",
    "            StructField(\"salario\", DoubleType(), False),\n",
    "            StructField(\"sistema_operacional_id\", IntegerType(), False),\n",
    "            StructField(\"pais_id\", IntegerType(), False),\n",
    "            StructField(\"empresa_id\", IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, when, lit\n",
    "from pyspark.sql.window import Window  \n",
    "\n",
    "class IntermediaryEntity(Ingestor):\n",
    "    def __init__(self, source, model, field, options, database='default', table='table', sink='HiveDAO',\n",
    "                 embbebedList=False):\n",
    "        self.metadata = dict({\n",
    "            'source': source,\n",
    "            'database': database,\n",
    "            'table': table,\n",
    "            'sink': sink,\n",
    "            'model': model,\n",
    "            'embbebedList': embbebedList,\n",
    "            'description_field': field,\n",
    "            'options': options\n",
    "        })\n",
    "\n",
    "    def save(self):\n",
    "        data = Ingestor.apply(IntermediaryEntity.rule, self.metadata)\n",
    "        self.load(self.metadata, data)\n",
    "\n",
    "    def get(self):\n",
    "        return Ingestor.apply(IntermediaryEntity.rule, self.metadata)\n",
    "\n",
    "    @staticmethod\n",
    "    def checkConstraint(field, dataframe):\n",
    "        \"\"\" Responsible for verifying the field's integrity given dataframe\"\"\"\n",
    "        return dataframe.\\\n",
    "            withColumn(field, when(col(field).isNull(), lit('Not Specified'))\\\n",
    "            .otherwise(col(field)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_index(columns, source):\n",
    "        return map(lambda x, y: (x, y.asDict()[columns]), range(1, len(source)), source)\n",
    "    \n",
    "    @staticmethod\n",
    "    def createIndex(ordered_column, index, source):\n",
    "        w = Window.orderBy(asc(ordered_column))\n",
    "        return source.withColumn(index, row_number().over(w))\n",
    "    \n",
    "    @udf()\n",
    "    def conversor(field):\n",
    "        try:\n",
    "            return MAPPER.value[field]\n",
    "        except KeyError as err:\n",
    "            return field\n",
    "    \n",
    "    @staticmethod\n",
    "    def rule(metadata):\n",
    "        \"\"\" Responsible to implement business logic to dimensional tables \"\"\"\n",
    "        model = metadata.get('model')\n",
    "        options = metadata.get('options')\n",
    "        field = metadata.get('description_field')\n",
    "        table = options.get('relationTable')\n",
    "        database = options.get(\"relationDatabase\")\n",
    "        print(options.get(\"originalColumn\"))\n",
    "        dimension = PostgresDAO().select(f'{database}.{table}')\n",
    "        \n",
    "        if metadata.get('embbebedList') == False:\n",
    "            data = Dimension.checkConstraint(field=field, dataframe=metadata.get('source'))\\\n",
    "                .select(options.get('intermediaryColumns'))\\\n",
    "                .join(dimension, col(options.get('originalColumn')) == getattr(dimension, \\\n",
    "                                                                                   options.get(\"dimensionalColumn\")))\n",
    "            data = data.withColumnRenamed(options.get('relationTableId'), options.get('relationTableId'))\n",
    "            return IntermediaryEntity.createIndex(options.get('originalIdColumn'), options.get('relationTbID'), data)\n",
    "        else:\n",
    "            data = Dimension.checkConstraint(field=field, dataframe=metadata.get('source'))\\\n",
    "                .select(options.get('intermediaryColumns'))\\\n",
    "                .withColumn(field, split(col(options.get(\"originalColumn\")), ';'))\\\n",
    "                .withColumn(field, explode(col(options.get(\"originalColumn\"))))\\\n",
    "                .join(dimension, col(options.get('originalColumn')) == getattr(dimension, \\\n",
    "                                                                                   options.get(\"dimensionalColumn\")))\n",
    "            data = data.withColumnRenamed(options.get('originalIdColumn'), options.get('relationTableId'))\n",
    "            data = data.withColumn(options.get('relationTableId'), col(options.get('relationTableId')).cast(IntegerType()))\n",
    "            data = IntermediaryEntity.createIndex(options.get('relationTableId'), options.get('relationTbID'), data)\n",
    "            if options.get(\"hasAdicionalColumns\") == True:\n",
    "                for i in options.get(\"adicionalColumns\"):\n",
    "                    setup = options.get(\"adicionalColumnsMapper\")[i]\n",
    "                    data = data.withColumnRenamed(i, setup[\"name\"])\n",
    "                    if setup.get(\"hasValueConversion\") == True:\n",
    "                        global MAPPER\n",
    "                        MAPPER = spark.sparkContext.broadcast(setup[\"valuesConversor\"])\n",
    "                        data = data.withColumn(setup['name'], IntermediaryEntity.conversor(setup['name']))\n",
    "                    data = data.withColumn(setup['name'], col(setup['name']).cast(setup['type']))\n",
    "            return data.select(model().schema.fieldNames())\n",
    "        \n",
    "    @staticmethod\n",
    "    def build(datasource, metadata):\n",
    "        return IntermediaryEntity(source=datasource,\n",
    "            model=metadata.get('model'),\n",
    "            field=metadata.get('field'),\n",
    "            database=metadata.get('database'),\n",
    "            table=metadata.get('table'),\n",
    "            embbebedList=metadata.get('embbebedList'),\n",
    "            sink=metadata.get('sink'),\n",
    "            options=metadata.get(\"options\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasource' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-483b7a1058e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                'relationTable': 'linguagem_programacao', 'relationDatabase': 'stackoverflow'})\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m df = IntermediaryEntity(source=datasource,\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRespLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datasource' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from requests import Session\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.models import Response\n",
    "\n",
    "class ExchangeDAO(DAO):\n",
    "    \n",
    "    def __init__(self, version=\"v4\", currency=\"BRL\", result='rates', **kwargs):\n",
    "        self.response_data = result\n",
    "        self.sleep = kwargs.get('sleep')\n",
    "        self.retries = kwargs.get('retries')\n",
    "        self.timeout = kwargs.get('timeout')\n",
    "        self.backoff_factor = kwargs.get('backoff_factor')\n",
    "        \n",
    "        self.endpoint = f'https://api.exchangerate-api.com/{version}/latest/{currency}'\n",
    "        self.session = ExchangeDAO.__requestSetup(self.retries, self.sleep)\n",
    "    \n",
    "    def __transform(self, data):\n",
    "        \"\"\"Impõe valor de R$ 3.81 para o dolar conforme Regra de Negócio\"\"\"\n",
    "        data = {key: 1/value for key, value in json.loads(data.text)[self.response_data].items()}\n",
    "        data.update({'USD': 3.81})\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def __requestSetup(retries, sleep):\n",
    "        session = Session()\n",
    "        retry = Retry(total=retries, read=retries, backoff_factor=sleep)\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        session.headers = {'Content-type': 'application/json'}\n",
    "        session.mount('http://', adapter)\n",
    "        return session\n",
    "\n",
    "    def collect(self):\n",
    "        \"\"\"Carrega os valores atuais de cada moeda, aplicando as \n",
    "        transformações necessárias para disponibilização dos dados\"\"\"\n",
    "        try:\n",
    "            request = ExchangeDAO.__requestSetup(retries=self.retries, sleep=self.sleep)\n",
    "            data = request.get(self.endpoint, timeout=self.timeout)\n",
    "            return self.__transform(data)\n",
    "        except Exception as err:\n",
    "            \"\"\"TODO\"\"\"\n",
    "            raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactRules(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def removeNullable(metadata, field, in_null_cases='Not Specified'):\n",
    "        \"\"\" Responsible for verifying the field's integrity given dataframe \"\"\"\n",
    "        return metadata.get('source').\\\n",
    "            withColumn(field, when(col(field).isNull(), lit(in_null_cases))\\\n",
    "            .otherwise(col(field)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def salary_standardization(metadata):\n",
    "        return metadata.get('source')\\\n",
    "        .withColumn(\"ConvertedSalary\", col('ConvertedSalary')/12)\\\n",
    "        .na.fill({'ConvertedSalary': 0})\\\n",
    "        .withColumn(\"ConvertedSalary\", exchange_conversion(\"CurrencySymbol\",\"ConvertedSalary\"))\\\n",
    "        .withColumn(\"ConvertedSalary\", col('ConvertedSalary').cast(DoubleType()))\\\n",
    "        \n",
    "            \n",
    "    @staticmethod\n",
    "    def respondent_name_creation(metadata):\n",
    "        return metadata.get('source').withColumn('nome', concat(lit('respondente_'), col('Respondent')))\n",
    "    \n",
    "    @staticmethod\n",
    "    def columns_renames(metadata):\n",
    "        return metadata.get('source')\\\n",
    "        .withColumnRenamed(\"ConvertedSalary\", \"salario\")\\\n",
    "        .withColumnRenamed(\"OpenSource\", \"contrib_open_source\")\\\n",
    "        .withColumnRenamed(\"Hobby\", \"programa_hobby\")\\\n",
    "        .withColumnRenamed(\"Respondent\", \"respondente_id\")\\\n",
    "        .withColumn(\"respondente_id\", col(\"respondente_id\").cast(IntegerType()))\n",
    "    \n",
    "    @staticmethod\n",
    "    def treat_embbebed_list(metadata):\n",
    "        return metadata.get(\"source\")\\\n",
    "            .withColumn(\"LanguageWorkedWith\", split(col(\"LanguageWorkedWith\"), ';'))\\\n",
    "            .withColumn(\"LanguageWorkedWith\", explode(col(\"LanguageWorkedWith\")))\\\n",
    "            .withColumn(\"CommunicationTools\", split(col(\"CommunicationTools\"), ';'))\\\n",
    "            .withColumn(\"CommunicationTools\", explode(col(\"CommunicationTools\")))\n",
    "\n",
    "    @staticmethod\n",
    "    def fact_enrichment(metadata):\n",
    "        pais = PostgresDAO().select('stackoverflow.pais').withColumnRenamed(\"nome\", \"pais_nome\")\n",
    "        sistema_operacional = PostgresDAO().select('stackoverflow.sistema_operacional').withColumnRenamed(\"nome\", \"so_nome\")\n",
    "        empresa = PostgresDAO().select('stackoverflow.empresa')\n",
    "\n",
    "        return metadata.get('source')\\\n",
    "        .join(pais, pais.pais_nome == col(\"Country\"))\\\n",
    "        .join(sistema_operacional, sistema_operacional.so_nome == col(\"OperatingSystem\"))\\\n",
    "        .join(empresa, empresa.tamanho == col(\"CompanySize\"))\n",
    "    \n",
    "    @staticmethod\n",
    "    def field_to_boolean(metadata):\n",
    "        return metadata.get('source')\\\n",
    "        .withColumn(\"contrib_open_source\", when(col(\"contrib_open_source\") == 'Yes', lit(1)).otherwise(lit(0)))\\\n",
    "        .withColumn(\"programa_hobby\", when(col(\"programa_hobby\") == 'Yes', lit(1)).otherwise(lit(0)))\\\n",
    "        .withColumn(\"salario\", col(\"salario\").cast(DoubleType()))\n",
    "\n",
    "@udf()\n",
    "def exchange_conversion(currencySymbol, salary):\n",
    "    global currencies\n",
    "    defaul_value = 0.0\n",
    "    try:\n",
    "        return currencies.value[currencySymbol] * salary if salary is not None else defaul_value\n",
    "    except KeyError as err:\n",
    "        return defaul_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "currencies = spark.sparkContext.broadcast(ExchangeDAO(sleep=0.5, retries=2, timeout=5).collect())\n",
    "\n",
    "class Fact(Ingestor):    \n",
    "    def __init__(self, source, model, database='default', table='table', sink='HiveDAO'):\n",
    "        self.metadata = dict({\n",
    "            'source': source,\n",
    "            'database': database,\n",
    "            'table': table,\n",
    "            'sink': sink,\n",
    "            'model': model})\n",
    "\n",
    "    def get(self):\n",
    "        self.update_source(Ingestor.apply(FactRules.salary_standardization, self.metadata))\n",
    "        self.update_source(Ingestor.apply(FactRules.respondent_name_creation, self.metadata))\n",
    "        self.update_source(Ingestor.apply(FactRules.columns_renames, self.metadata))\n",
    "        \n",
    "        self.update_source(FactRules.removeNullable(self.metadata, \"CommunicationTools\"))\n",
    "        self.update_source(FactRules.removeNullable(self.metadata, \"OperatingSystem\"))\n",
    "        self.update_source(FactRules.removeNullable(self.metadata, \"LanguageWorkedWith\"))\n",
    "        self.update_source(FactRules.removeNullable(self.metadata, \"CompanySize\"))\n",
    "        self.update_source(FactRules.removeNullable(self.metadata, \"Country\"))\n",
    "        self.update_source(Ingestor.apply(FactRules.treat_embbebed_list, self.metadata))\n",
    "        self.update_source(Ingestor.apply(FactRules.fact_enrichment, self.metadata))\n",
    "        self.update_source(Ingestor.apply(FactRules.field_to_boolean, self.metadata))\n",
    "        return self.filter_schema()\n",
    "    \n",
    "    def filter_schema(self):\n",
    "        model = self.metadata.get('model')\n",
    "        return self.metadata.get('source').select(model().schema.fieldNames()).distinct()\n",
    "    \n",
    "    def save(self):\n",
    "        data = self.get()\n",
    "        self.load(self.metadata, data)\n",
    "    \n",
    "    def update_source(self, data):\n",
    "        self.metadata.update({'source': data})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Entrypoint </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o12545.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 507.0 failed 1 times, most recent failure: Lost task 0.0 in stage 507.0 (TID 26008, localhost, executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow.pais (\"pais_id\",\"nome\") VALUES (1,'Paraguay') was aborted: ERROR: duplicate key value violates unique constraint \"pais_pkey\"\n",
      "  Detail: Key (pais_id)=(1) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:155)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:508)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:836)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:935)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"pais_pkey\"\n",
      "  Detail: Key (pais_id)=(1) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:274)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat sun.reflect.GeneratedMethodAccessor180.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow.pais (\"pais_id\",\"nome\") VALUES (1,'Paraguay') was aborted: ERROR: duplicate key value violates unique constraint \"pais_pkey\"\n",
      "  Detail: Key (pais_id)=(1) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:155)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:508)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:836)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:935)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"pais_pkey\"\n",
      "  Detail: Key (pais_id)=(1) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
      "\t... 18 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o12643.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 510.0 failed 1 times, most recent failure: Lost task 7.0 in stage 510.0 (TID 26229, localhost, executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow.ferramenta_comunic (\"ferramenta_comunic_id\",\"nome\") VALUES (11,'Facebook') was aborted: ERROR: duplicate key value violates unique constraint \"ferramenta_comunic_pkey\"\n",
      "  Detail: Key (ferramenta_comunic_id)=(11) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:155)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:508)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:836)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:935)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"ferramenta_comunic_pkey\"\n",
      "  Detail: Key (ferramenta_comunic_id)=(11) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:274)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat sun.reflect.GeneratedMethodAccessor180.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow.ferramenta_comunic (\"ferramenta_comunic_id\",\"nome\") VALUES (11,'Facebook') was aborted: ERROR: duplicate key value violates unique constraint \"ferramenta_comunic_pkey\"\n",
      "  Detail: Key (ferramenta_comunic_id)=(11) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:155)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:508)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:836)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:935)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"ferramenta_comunic_pkey\"\n",
      "  Detail: Key (ferramenta_comunic_id)=(11) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
      "\t... 18 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o12741.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 513.0 failed 1 times, most recent failure: Lost task 1.0 in stage 513.0 (TID 26437, localhost, executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow.linguagem_programacao (\"linguagem_programacao_id\",\"nome\") VALUES (5,'Bash/Shell') was aborted: ERROR: duplicate key value violates unique constraint \"linguagem_programacao_pkey\"\n",
      "  Detail: Key (linguagem_programacao_id)=(5) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:155)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:508)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:836)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:935)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"linguagem_programacao_pkey\"\n",
      "  Detail: Key (linguagem_programacao_id)=(5) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:274)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat sun.reflect.GeneratedMethodAccessor180.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow.linguagem_programacao (\"linguagem_programacao_id\",\"nome\") VALUES (5,'Bash/Shell') was aborted: ERROR: duplicate key value violates unique constraint \"linguagem_programacao_pkey\"\n",
      "  Detail: Key (linguagem_programacao_id)=(5) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:155)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:508)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:836)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:935)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"linguagem_programacao_pkey\"\n",
      "  Detail: Key (linguagem_programacao_id)=(5) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
      "\t... 18 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o12833.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 516.0 failed 1 times, most recent failure: Lost task 7.0 in stage 516.0 (TID 26657, localhost, executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow.sistema_operacional (\"sistema_operacional_id\",\"nome\") VALUES (5,'Windows') was aborted: ERROR: duplicate key value violates unique constraint \"sistema_operacional_pkey\"\n",
      "  Detail: Key (sistema_operacional_id)=(5) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:155)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:50)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:508)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:836)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:935)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"sistema_operacional_pkey\"\n",
      "  Detail: Key (sistema_operacional_id)=(5) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:274)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat sun.reflect.GeneratedMethodAccessor180.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow.sistema_operacional (\"sistema_operacional_id\",\"nome\") VALUES (5,'Windows') was aborted: ERROR: duplicate key value violates unique constraint \"sistema_operacional_pkey\"\n",
      "  Detail: Key (sistema_operacional_id)=(5) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:155)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:50)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:508)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:836)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:935)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"sistema_operacional_pkey\"\n",
      "  Detail: Key (sistema_operacional_id)=(5) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
      "\t... 18 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error occurred while calling o12925.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 519.0 failed 1 times, most recent failure: Lost task 5.0 in stage 519.0 (TID 26869, localhost, executor driver): java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow.empresa (\"empresa_id\",\"tamanho\") VALUES (6,'500 to 999 employees') was aborted: ERROR: duplicate key value violates unique constraint \"empresa_pkey\"\n",
      "  Detail: Key (empresa_id)=(6) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:155)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:50)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:508)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:836)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:935)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"empresa_pkey\"\n",
      "  Detail: Key (empresa_id)=(6) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)\n",
      "\tat scala.Option.foreach(Option.scala:274)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat sun.reflect.GeneratedMethodAccessor180.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.sql.BatchUpdateException: Batch entry 0 INSERT INTO stackoverflow.empresa (\"empresa_id\",\"tamanho\") VALUES (6,'500 to 999 employees') was aborted: ERROR: duplicate key value violates unique constraint \"empresa_pkey\"\n",
      "  Detail: Key (empresa_id)=(6) already exists.  Call getNextException to see other errors in the batch.\n",
      "\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:155)\n",
      "\tat org.postgresql.core.ResultHandlerDelegate.handleError(ResultHandlerDelegate.java:50)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:508)\n",
      "\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n",
      "\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n",
      "\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:672)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:836)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:834)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:935)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:935)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"empresa_pkey\"\n",
      "  Detail: Key (empresa_id)=(6) already exists.\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n",
      "\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n",
      "\t... 18 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasource = DAO.from_csv(\"base_de_respostas_10k_amostra.csv\")\n",
    "\n",
    "Dimension(source=datasource,\n",
    "            model=CountryModel,\n",
    "            field=\"Country\",\n",
    "            database=\"stackoverflow\",\n",
    "            table=\"pais\",\n",
    "            embbebedList=False,\n",
    "            sink='PostgresDAO').save()\n",
    "\n",
    "Dimension(source=datasource,\n",
    "            model=CommunicationToolModel,\n",
    "            field=\"CommunicationTools\",\n",
    "            database=\"stackoverflow\",\n",
    "            table=\"ferramenta_comunic\",\n",
    "            embbebedList=True,\n",
    "            sink='PostgresDAO').save()\n",
    "\n",
    "Dimension(source=datasource,\n",
    "            model=LanguageWorkedWithModel,\n",
    "            field=\"LanguageWorkedWith\",\n",
    "            database=\"stackoverflow\",\n",
    "            table=\"linguagem_programacao\",\n",
    "            embbebedList=True,\n",
    "            sink='PostgresDAO').save()\n",
    "\n",
    "Dimension(source=datasource,\n",
    "            model=SOModel,\n",
    "            field=\"OperatingSystem\",\n",
    "            database=\"stackoverflow\",\n",
    "            table=\"sistema_operacional\",\n",
    "            embbebedList=False,\n",
    "            sink='PostgresDAO').save()\n",
    "\n",
    "Dimension(source=datasource,\n",
    "            model=CompanyModel,\n",
    "            field=\"CompanySize\",\n",
    "            database=\"stackoverflow\",\n",
    "            table=\"empresa\",\n",
    "            embbebedList=False,\n",
    "            sink='PostgresDAO').save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageWorkedWith\n",
      "CommunicationTools\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = Fact(source=datasource,\n",
    "            model=RespondentModel,\n",
    "            database=\"stackoverflow\",\n",
    "            table=\"respondente\",\n",
    "            sink='PostgresDAO').save()\n",
    "\n",
    "options = dict({\"originalIdColumn\": \"Respondent\", \"relationTableId\":\"respondente_id\",\n",
    "                \"intermediaryColumns\": ['Respondent', 'LanguageWorkedWith', 'Hobby'],\n",
    "                \"hasAdicionalColumns\": True,\n",
    "                \"adicionalColumns\": [\"Hobby\"],\n",
    "                \"adicionalColumnsMapper\": {\"Hobby\":{\"name\": \"momento\", \"hasValueConversion\": True,\n",
    "                                                    \"valuesConversor\": {\"No\": 0, \"Yes\": 1},\n",
    "                                                        \"type\": IntegerType()}},\n",
    "                'originalColumn': 'LanguageWorkedWith',\n",
    "                'dimensionalColumn': 'nome', \"relationTbID\":\"resp_usa_linguagem_id\",\n",
    "               'relationTable': 'linguagem_programacao', 'relationDatabase': 'stackoverflow'})\n",
    "\n",
    "df = IntermediaryEntity(source=datasource,\n",
    "            model=RespLanguageModel,\n",
    "            options=options,\n",
    "            field=\"LanguageWorkedWith\",\n",
    "            database=\"stackoverflow\",\n",
    "            table=\"resp_usa_linguagem\",\n",
    "            embbebedList=True,\n",
    "            sink='PostgresDAO').save()\n",
    "\n",
    "options = dict({\"originalIdColumn\": \"Respondent\", \"relationTableId\":\"respondente_id\",\n",
    "                \"intermediaryColumns\": ['Respondent', 'CommunicationTools'], \n",
    "                'originalColumn': 'CommunicationTools',\n",
    "                'dimensionalColumn': 'nome', \"relationTbID\":\"resp_usa_ferramenta_id\",\n",
    "                'relationTable': 'ferramenta_comunic', 'relationDatabase': 'stackoverflow'})\n",
    "\n",
    "df = IntermediaryEntity(source=datasource,\n",
    "            model=RespToolsModel,\n",
    "            options=options,\n",
    "            field=\"CommunicationTools\",\n",
    "            database=\"stackoverflow\",\n",
    "            table=\"resp_usa_ferramenta\",\n",
    "            embbebedList=True,\n",
    "            sink='PostgresDAO').save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
